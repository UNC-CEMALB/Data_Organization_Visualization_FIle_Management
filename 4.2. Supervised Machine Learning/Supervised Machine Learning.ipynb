{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dad7d069",
   "metadata": {},
   "source": [
    "# Supervised Machine Learning \n",
    "\n",
    "This training module was developed by Oyemwenosa N. Avenbuan, Alexis Payton, MS, and Dr. Julia E. Rager\n",
    "\n",
    "Spring 2023\n",
    "\n",
    "## Introduction to Training Module\n",
    "Machine learning is a field that has been around for decades, but has exploded in popularity and utility in recent years due to proliferation of big data. Through the building of models, machine learning has the ability to sift through and learn from large volumes of data and use that knowledge to solve problems. The challenges of big and high dimensional data as they pertain to environmental health and how machine learning can mitigate some of those challenges are discussed further in [Payton et. al](https://www.frontiersin.org/articles/10.3389/ftox.2023.1171175/full).\n",
    "\n",
    "In this training module, we will explore:\n",
    "+ Types of machine learning\n",
    "+ Building/ training a supervised machine learning algorithm\n",
    "+ Assessment of a supervised machine learning model's performance\n",
    "\n",
    "Supervised machine learning will be explored using a geology dataset. This example dataset was generated by measuring inorganic Arsenic (iAs) concentrations and other variables in 713 private wells across North Carolina. Let's go ahead and view these data. \n",
    "\n",
    "### Installing required R packages\n",
    "If you already have these packages installed, you can skip this step, or you can run the below code which checks installation status for you"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95d5cf47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading required namespace: readxl\n",
      "\n",
      "Loading required namespace: lubridate\n",
      "\n",
      "Loading required namespace: tidyverse\n",
      "\n",
      "Loading required namespace: gtsummary\n",
      "\n",
      "Loading required namespace: caret\n",
      "\n",
      "Loading required namespace: e1071\n",
      "\n",
      "Loading required namespace: Hmsic\n",
      "\n",
      "Warning message:\n",
      "“package ‘Hmsic’ is not available for this version of R\n",
      "\n",
      "A version of this package for your version of R might be available elsewhere,\n",
      "see the ideas at\n",
      "https://cran.r-project.org/doc/manuals/r-patched/R-admin.html#Installing-packages”\n",
      "Loading required namespace: randomForest\n",
      "\n",
      "Loading required namespace: themis\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if (!requireNamespace(\"readxl\"))\n",
    "  install.packages(\"readxl\");\n",
    "if (!requireNamespace(\"lubridate\"))\n",
    "  install.packages(\"lubridate\");\n",
    "if (!requireNamespace(\"tidyverse\"))\n",
    "  install.packages(\"tidyverse\");\n",
    "if (!requireNamespace(\"gtsummary\"))\n",
    "  install.packages(\"gtsummary\");\n",
    "if (!requireNamespace(\"caret\"))\n",
    "  install.packages(\"caret\");\n",
    "if (!requireNamespace(\"e1071\"))\n",
    "  install.packages(\"e1071\");\n",
    "if (!requireNamespace(\"Hmsic\"))\n",
    "  install.packages(\"Hmsic\");\n",
    "if (!requireNamespace(\"randomForest\"))\n",
    "  install.packages(\"randomForest\");\n",
    "if (!requireNamespace(\"pROC\"))\n",
    "  install.packages(\"pROC\");\n",
    "if (!requireNamespace(\"themis\"))\n",
    "  install.packages(\"themis\");\n",
    "if (!requireNamespace(\"rlang\"))\n",
    "  install.packages(\"rlang\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6ff895",
   "metadata": {},
   "source": [
    "### Loading required R packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d30a7fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Attaching package: ‘lubridate’\n",
      "\n",
      "\n",
      "The following objects are masked from ‘package:base’:\n",
      "\n",
      "    date, intersect, setdiff, union\n",
      "\n",
      "\n",
      "── \u001b[1mAttaching core tidyverse packages\u001b[22m ──────────────────────── tidyverse 2.0.0 ──\n",
      "\u001b[32m✔\u001b[39m \u001b[34mdplyr  \u001b[39m 1.1.2     \u001b[32m✔\u001b[39m \u001b[34mreadr  \u001b[39m 2.1.4\n",
      "\u001b[32m✔\u001b[39m \u001b[34mforcats\u001b[39m 1.0.0     \u001b[32m✔\u001b[39m \u001b[34mstringr\u001b[39m 1.5.0\n",
      "\u001b[32m✔\u001b[39m \u001b[34mggplot2\u001b[39m 3.4.2     \u001b[32m✔\u001b[39m \u001b[34mtibble \u001b[39m 3.2.1\n",
      "\u001b[32m✔\u001b[39m \u001b[34mpurrr  \u001b[39m 1.0.1     \u001b[32m✔\u001b[39m \u001b[34mtidyr  \u001b[39m 1.3.0\n",
      "── \u001b[1mConflicts\u001b[22m ────────────────────────────────────────── tidyverse_conflicts() ──\n",
      "\u001b[31m✖\u001b[39m \u001b[34mdplyr\u001b[39m::\u001b[32mfilter()\u001b[39m masks \u001b[34mstats\u001b[39m::filter()\n",
      "\u001b[31m✖\u001b[39m \u001b[34mdplyr\u001b[39m::\u001b[32mlag()\u001b[39m    masks \u001b[34mstats\u001b[39m::lag()\n",
      "\u001b[36mℹ\u001b[39m Use the conflicted package (\u001b[3m\u001b[34m<http://conflicted.r-lib.org/>\u001b[39m\u001b[23m) to force all conflicts to become errors\n",
      "Loading required package: lattice\n",
      "\n",
      "\n",
      "Attaching package: ‘caret’\n",
      "\n",
      "\n",
      "The following object is masked from ‘package:purrr’:\n",
      "\n",
      "    lift\n",
      "\n",
      "\n",
      "\n",
      "Attaching package: ‘Hmisc’\n",
      "\n",
      "\n",
      "The following object is masked from ‘package:e1071’:\n",
      "\n",
      "    impute\n",
      "\n",
      "\n",
      "The following objects are masked from ‘package:dplyr’:\n",
      "\n",
      "    src, summarize\n",
      "\n",
      "\n",
      "The following objects are masked from ‘package:base’:\n",
      "\n",
      "    format.pval, units\n",
      "\n",
      "\n",
      "randomForest 4.7-1.1\n",
      "\n",
      "Type rfNews() to see new features/changes/bug fixes.\n",
      "\n",
      "\n",
      "Attaching package: ‘randomForest’\n",
      "\n",
      "\n",
      "The following object is masked from ‘package:dplyr’:\n",
      "\n",
      "    combine\n",
      "\n",
      "\n",
      "The following object is masked from ‘package:ggplot2’:\n",
      "\n",
      "    margin\n",
      "\n",
      "\n",
      "Type 'citation(\"pROC\")' for a citation.\n",
      "\n",
      "\n",
      "Attaching package: ‘pROC’\n",
      "\n",
      "\n",
      "The following objects are masked from ‘package:stats’:\n",
      "\n",
      "    cov, smooth, var\n",
      "\n",
      "\n",
      "Loading required package: recipes\n",
      "\n",
      "\n",
      "Attaching package: ‘recipes’\n",
      "\n",
      "\n",
      "The following objects are masked from ‘package:gtsummary’:\n",
      "\n",
      "    all_double, all_factor, all_integer, all_logical, all_numeric\n",
      "\n",
      "\n",
      "The following object is masked from ‘package:stringr’:\n",
      "\n",
      "    fixed\n",
      "\n",
      "\n",
      "The following object is masked from ‘package:stats’:\n",
      "\n",
      "    step\n",
      "\n",
      "\n",
      "\n",
      "Attaching package: ‘rlang’\n",
      "\n",
      "\n",
      "The following objects are masked from ‘package:purrr’:\n",
      "\n",
      "    %@%, flatten, flatten_chr, flatten_dbl, flatten_int, flatten_lgl,\n",
      "    flatten_raw, invoke, splice\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "library(readxl);\n",
    "library(lubridate);\n",
    "library(tidyverse);\n",
    "library(gtsummary);\n",
    "library(caret);\n",
    "library(e1071);\n",
    "library(Hmisc);\n",
    "library(randomForest);\n",
    "library(pROC);\n",
    "library(themis);\n",
    "library(rlang);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5445cf",
   "metadata": {},
   "source": [
    "### Set your working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfcf387",
   "metadata": {},
   "outputs": [],
   "source": [
    "setwd(\"/filepath to where your input files are\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0440d6",
   "metadata": {},
   "source": [
    "### Importing example dataset\n",
    "**Will need to change input to module number and add module number to the file itself**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6e0d612c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A data.frame: 6 × 8</caption>\n",
       "<thead>\n",
       "\t<tr><th></th><th scope=col>Tax_ID</th><th scope=col>Water_Sample_Date</th><th scope=col>Casing_Depth</th><th scope=col>Well_Depth</th><th scope=col>Static_Water_Depth</th><th scope=col>Flow_Rate</th><th scope=col>pH</th><th scope=col>Detect_Concentration</th></tr>\n",
       "\t<tr><th></th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;chr&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>1</th><td>1006004</td><td>9/24/12 </td><td>52</td><td>165</td><td>41</td><td>60.0</td><td>7.7</td><td>ND</td></tr>\n",
       "\t<tr><th scope=row>2</th><td>1024009</td><td>12/17/15</td><td>40</td><td>445</td><td>42</td><td> 2.0</td><td>7.3</td><td>ND</td></tr>\n",
       "\t<tr><th scope=row>3</th><td>1054019</td><td>2/2/15  </td><td>45</td><td>160</td><td>40</td><td>40.0</td><td>7.4</td><td>ND</td></tr>\n",
       "\t<tr><th scope=row>4</th><td>1057017</td><td>10/22/12</td><td>42</td><td>440</td><td>57</td><td> 1.5</td><td>8.0</td><td>D </td></tr>\n",
       "\t<tr><th scope=row>5</th><td>1060006</td><td>1/3/11  </td><td>48</td><td>120</td><td>42</td><td>25.0</td><td>7.1</td><td>ND</td></tr>\n",
       "\t<tr><th scope=row>6</th><td>1066006</td><td>12/15/15</td><td>60</td><td>280</td><td>32</td><td>10.0</td><td>8.2</td><td>D </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A data.frame: 6 × 8\n",
       "\\begin{tabular}{r|llllllll}\n",
       "  & Tax\\_ID & Water\\_Sample\\_Date & Casing\\_Depth & Well\\_Depth & Static\\_Water\\_Depth & Flow\\_Rate & pH & Detect\\_Concentration\\\\\n",
       "  & <chr> & <chr> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <chr>\\\\\n",
       "\\hline\n",
       "\t1 & 1006004 & 9/24/12  & 52 & 165 & 41 & 60.0 & 7.7 & ND\\\\\n",
       "\t2 & 1024009 & 12/17/15 & 40 & 445 & 42 &  2.0 & 7.3 & ND\\\\\n",
       "\t3 & 1054019 & 2/2/15   & 45 & 160 & 40 & 40.0 & 7.4 & ND\\\\\n",
       "\t4 & 1057017 & 10/22/12 & 42 & 440 & 57 &  1.5 & 8.0 & D \\\\\n",
       "\t5 & 1060006 & 1/3/11   & 48 & 120 & 42 & 25.0 & 7.1 & ND\\\\\n",
       "\t6 & 1066006 & 12/15/15 & 60 & 280 & 32 & 10.0 & 8.2 & D \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A data.frame: 6 × 8\n",
       "\n",
       "| <!--/--> | Tax_ID &lt;chr&gt; | Water_Sample_Date &lt;chr&gt; | Casing_Depth &lt;dbl&gt; | Well_Depth &lt;dbl&gt; | Static_Water_Depth &lt;dbl&gt; | Flow_Rate &lt;dbl&gt; | pH &lt;dbl&gt; | Detect_Concentration &lt;chr&gt; |\n",
       "|---|---|---|---|---|---|---|---|---|\n",
       "| 1 | 1006004 | 9/24/12  | 52 | 165 | 41 | 60.0 | 7.7 | ND |\n",
       "| 2 | 1024009 | 12/17/15 | 40 | 445 | 42 |  2.0 | 7.3 | ND |\n",
       "| 3 | 1054019 | 2/2/15   | 45 | 160 | 40 | 40.0 | 7.4 | ND |\n",
       "| 4 | 1057017 | 10/22/12 | 42 | 440 | 57 |  1.5 | 8.0 | D  |\n",
       "| 5 | 1060006 | 1/3/11   | 48 | 120 | 42 | 25.0 | 7.1 | ND |\n",
       "| 6 | 1066006 | 12/15/15 | 60 | 280 | 32 | 10.0 | 8.2 | D  |\n",
       "\n"
      ],
      "text/plain": [
       "  Tax_ID  Water_Sample_Date Casing_Depth Well_Depth Static_Water_Depth\n",
       "1 1006004 9/24/12           52           165        41                \n",
       "2 1024009 12/17/15          40           445        42                \n",
       "3 1054019 2/2/15            45           160        40                \n",
       "4 1057017 10/22/12          42           440        57                \n",
       "5 1060006 1/3/11            48           120        42                \n",
       "6 1066006 12/15/15          60           280        32                \n",
       "  Flow_Rate pH  Detect_Concentration\n",
       "1 60.0      7.7 ND                  \n",
       "2  2.0      7.3 ND                  \n",
       "3 40.0      7.4 ND                  \n",
       "4  1.5      8.0 D                   \n",
       "5 25.0      7.1 ND                  \n",
       "6 10.0      8.2 D                   "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the data\n",
    "well_data <- data.frame(read_excel(\"Module5/Well_Data.xlsx\"))\n",
    "\n",
    "# View the top of the dataset\n",
    "head(well_data) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc320e75",
   "metadata": {},
   "source": [
    "The columns in this dataset are described below:\n",
    "+ `Tax_ID`: Tax ID for the property\n",
    "+ `Water_Sample_ID`: Date that the well was sampled \n",
    "+ `Casing_Depth`: Depth of the casing of the well (ft)\n",
    "+ `Well_Depth`: Depth of the well (ft)\n",
    "+ `Static_Water_Depth`: Static water depth in the well (ft)\n",
    "+ `Flow_Rate`: Well flow rate (gallons per minute)\n",
    "+ `pH`: pH of water sample\n",
    "+ `Detect_Concentration`: Binary identifier (either non-detect (ND) or detect (D)) if As concentration detected in water sample \n",
    "\n",
    "\n",
    "## Training Module's Environmental Health Questions\n",
    "\n",
    "This training module was specifically developed to answer the following environmental health questions:\n",
    "\n",
    "1. Can we predict if iAs will be detected based on well water data? \n",
    "\n",
    "Before we dive into the code, let's get an understanding for what machine learning is, how to train a model, and assess its model performance.\n",
    "\n",
    "## What is Machine Learning?\n",
    "\n",
    "Machine learning is a field of study in computer science that involves creating algorithms, which are a set of instructions that perform a specific task on a given dataset. These algorithms enable researchers to create models that can automatically analyze to new and unforeseen situations capable of improving automatically through experience and data.\n",
    "\n",
    "In other words, instead of being explicitly programmed to perform a task, a machine learning algorithm is designed to learn from examples and data, allowing it to adapt and improve over time. This approach is particularly useful for tasks that are too complex or difficult to be solved using traditional programming methods. **NOSA - I'M NOT SURE WHAT YOU MEANT HERE BY TRADITIONAL PROGRAMMING METHODS**\n",
    "\n",
    "Through machine learning, scientists can:\n",
    "\n",
    "+ Create a model that adapts to new circumstances \n",
    "+ Detect patterns in large and complex datasets\n",
    "+ Evaluate the effectiveness of these patterns \n",
    "+ Make informed decisions about how to improve their models\n",
    "\n",
    "\n",
    "## Types of Machine Learning\n",
    "\n",
    "Within the field of machine learning, there are two types that are most commonly used in environmental toxicology research: supervised machine learning and unsupervised machine learning.\n",
    "\n",
    "**Supervised machine learning** involves training a model using a labeled dataset, where each dependent or predictor variable is associated with an independent variable with a known outcome. This allows the model to learn how to predict the labeled outcome on data it hasn't \"seen\" before based on the patterns and relationships it previously identified in the data. For example, supervised machine learning has been used for cancer prediction and prognosis based on variables like tumor size, stage, and age ([Lynch et. al](https://www.sciencedirect.com/science/article/abs/pii/S1386505617302368?via%3Dihub), [Asadi et. al](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7416093/)). \n",
    "\n",
    "Supervised machine learning includes: \n",
    "+ Classification: Using algorithms to classify a categorical outcome (ie. plant species, disease status, etc.)\n",
    "+ Regression: Using algorithms to predict a continuous outcome (ie. temperature, chemical concentration, etc.)\n",
    "<img src=\"https://github.com/UNC-CEMALB/P1006_Data-Organization-for-High-Dimensional-Analyses-in-Environmental-Health/assets/69641855/3c22a8fc-ada5-4199-9967-77f504d99d2a\" width=\"684\" />\n",
    "\n",
    "([Soni, 2018](https://towardsdatascience.com/supervised-vs-unsupervised-learning-14f68e32ea8d))\n",
    "\n",
    "\n",
    "**Unsupervised machine learning**, on the other hand, involves using models to find patterns or associations between variables in dataset that lack a known or labeled outcome. For example, unsupervised machine learning has been used to find associations of co-expressed genes within various biological pathways ([Botía et. al](https://bmcsystbiol.biomedcentral.com/articles/10.1186/s12918-017-0420-6), [Pagnuco et. al](https://www.sciencedirect.com/science/article/pii/S0888754317300575?via%3Dihub)).\n",
    "\n",
    "<img src=\"https://github.com/UNC-CEMALB/P1006_Data-Organization-for-High-Dimensional-Analyses-in-Environmental-Health/assets/69641855/4f78adef-97b6-425f-9a51-43315b0fb7b2\" width=\"684\" />\n",
    "\n",
    "([Langs et. al](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6244522/))\n",
    "\n",
    "It's worth noting that there are also other types of machine learning, including semi-supervised learning, reinforcement learning and deep learning, which won't be discussed in these training modules. Overall, the distinction between supervised and unsupervised learning is an important concept in machine learning, as it can inform the choice of algorithms and techniques used to analyze and make predictions from data.\n",
    "\n",
    "To learn more check out the following resources: **I WILL LIKELY CHANGE THESE RESOURCES...I THINK WE CAN FIND BETTER ONES**\n",
    "\n",
    "+ [Machine Learning Mastery](https://machinelearningmastery.com/machine-learning-in-r-step-by-step/)\n",
    "+ [Master in Data Science](https://www.mastersindatascience.org/learning/machine-learning-algorithms/decision-tree/)\n",
    "+ [IBM - What is Machine Learning](https://www.ibm.com/topics/machine-learning)\n",
    "+ Machine Learning by Mueller, J. P. (2021). Machine learning for dummies. John Wiley &amp; Sons. \n",
    "\n",
    "## Training Supervised Machine Learning Models\n",
    "\n",
    "In supervised machine learning, algorithms need to be trained before they can be used to predict data. This involves selecting a smaller portion of data, known as training data, so that the model will learn how to predict the outcome as accurately as possible. The process of training an algorithm is essential for enabling it to learn and improve over time, allowing it to make more accurate predictions and better adapt to new and changing circumstances. Ultimately, the effectiveness of a machine learning model depends on the quality and relevance of its training data.\n",
    "\n",
    "Let's imagine you're interested in predicting an animal's species (either a cat or a dog) based on a dataset that contains variables regarding weight, height, coat color, ear shape, etc. These data can be divided into a training set and a test set. The **training set** is a subset of the data that the model will learn from to make associations between the predictor variables (ie. height, weight etc.) and the outcome (ie. cat or dog). The **test set** is used used to evaluate what the model has learned from the training set. \n",
    "\n",
    "Our dataset will look something like this with 60% of the data in the training set and 40% of the data in the test set:\n",
    "<img src=\"https://github.com/UNC-CEMALB/P1006_Data-Organization-for-High-Dimensional-Analyses-in-Environmental-Health/assets/69641855/a8c723ec-e50c-4cb9-aff5-4230d0d6fc2c\" width=\"684\"/>\n",
    "*Created with BioRender.com*\n",
    "\n",
    "\n",
    "The process of developing a model involves dividing the data into three main sets:\n",
    "\n",
    "1. **Training Set:** a subset of the data that the model uses to learn from the data by identifying patterns.\n",
    "\n",
    "2. **Validation Set**: a subset of data that is used to evaluate the model's fit in an unbiased way by fine-tuning its parameters and optimizing its performance. This is akin to pop quizzes that help students improve their understanding and performance. **I DON'T THINK WE SHOULD WORRY ABOUT THIS, BECAUSE 9/10 WE WON'T HAVE ENOUGH DATA TO CREATE THIS IN ENVIRONMENTAL HEALTH RELATED RESEARCH AND WE HAVEN'T CREATED CODE THAT INCLUDES THIS**\n",
    "\n",
    "2. **Test Set:** a subset of data that is used to evaluate the final model's fit based on the training and validation sets. This is the model's final exam, as it provides an objective assessment of the model's ability to generalize to new, unseen data.\n",
    "  \n",
    "\n",
    "It's important to note that the test set should only be used once, after the model has been trained using the training dataset. Using the test set multiple times during the development process can lead to overfitting, where the model performs well on the test data but poorly on new, unseen data. The ideal algorithm is generalizable or flexible enough to accurately predict unseen data. This is known as the bias-variance tradeoff. \n",
    "\n",
    "The last topic we should mention in this section is **cross validation** or **k-fold cross validation**. If the dataset is based on this 60:40 split that we mentioned earlier, our model's accuracy will likely be influenced based upon *where* this 60:40 split occurs in the dataset. Cross validation is implemented to ensure that the model is exposed to more patterns in the dataset, which reduces bias improving prediction accuracy. For example, if 5-fold cross validation were to be implemented, we would have 5 different folds or splits in the dataset with 4 retained for training and 1 retained for testing. These splits would occur 5 different times as shown in the figure below.\n",
    "\n",
    "**INSERT IMAGE OF 5 FOLD CV**\n",
    "\n",
    "Confusion matrix metrics would be calculated after each iteration and averaged for the final results. For additional information on the bias-variance tradeoff and the different types of cross validation check out the resources below. \n",
    "\n",
    "### Resources\n",
    "+ [Understanding the Bias-Variance Tradeoff](https://towardsdatascience.com/understanding-the-bias-variance-tradeoff-165e6942b229)\n",
    "+ [Cross Validation in Machine Learning](https://towardsdatascience.com/cross-validation-in-machine-learning-72924a69872f)\n",
    "+ [Cross Validation Pros & Cons](https://www.geeksforgeeks.org/cross-validation-machine-learning/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223a2a67",
   "metadata": {},
   "source": [
    "## Assessing Model Performance  \n",
    "Metrics from a confusion matrix are used to evalute model performance for classification-based supervised machine learning models. A confusion matrix consists of table that displays the numbers of how often the algorithm correctly or incorrectly predicted the outcome. \n",
    "\n",
    "Going back to our animal classification example, let's say the confusion matrix below is a result of how well the model was able to predict whether an animal was considered to be a cat or a dog.\n",
    "\n",
    "<img src=\"https://github.com/UNC-CEMALB/P1006_Data-Organization-for-High-Dimensional-Analyses-in-Environmental-Health/assets/69641855/4c7a90a0-725b-48b8-ada4-77a5907131a0\" width=\"684\" />\n",
    "\n",
    "*Created with BioRender.com*\n",
    "\n",
    "Some of the metrics that can be obtained from a confusion matrix are listed below:\n",
    "\n",
    "+ **Balanced Accuracy:** is the ratio of correct predictions (TP + TN) to the total number of predictions (TP + TN + TN + FN) and is typically used to assess overall model performance. This metric is prone to skew for imbalanced data. For example, if the animal dataset had 11 cats and 74 dogs the data would be considered to be imbalanced.\n",
    "\n",
    "+ **Sensitivity or Recall:** evaluates how well the model was able to predict the \"positive\" class. It's the ratio of correctly classified true positives to total number of all true positives (TP + FN). \n",
    "\n",
    "+ **Specificity:** evaluates how well the model was able to predict the \"negative\" class. It's the ratio of correctly classified true negatives to total number of all true negatives (TN + FP). \n",
    "\n",
    "+  **Positive Predictive Value (PPV) or Precision:**  evaluates how well the model was able to predict the \"positive\" class. It's the ratio of correctly classified true positives to total number of predicted positives (TP + FP).\n",
    "\n",
    "+  **Negative Predictive Value (PPV):**  evaluates how well the model was able to predict the \"negative\" class. It's the ratio of correctly classified true negatives to total number of predicted positives (TN + FN).\n",
    "\n",
    "For all metrics, the values fall between 0 and 1, where 0 represents the model not being able to classify any data points correctly and 1 representing that the model was able to classify all test data correctly. Typically a balanced accuracy of at least 0.7 is considered respectable.\n",
    "\n",
    "\n",
    "**Note**: For multiclass classification (more than two labeled outcomes to be predicted), the same metrics are used, but are obtained in a slightly different way. Regression based supervised machine learning models use loss functions to evaluate model performance. For more information regarding confusion matrices and loss functions for regression-based models check out the resources below.\n",
    "\n",
    "### Resources\n",
    " + [Additional Confusion Matrix Metrics](https://medium.com/analytics-vidhya/what-is-a-confusion-matrix-d1c0f8feda5)\n",
    " + [Precision vs. Recall or Specificity vs. Sensitivity](https://towardsdatascience.com/should-i-look-at-precision-recall-or-specificity-sensitivity-3946158aace1)\n",
    " + [Loss Functions for Machine Learning Regression](https://towardsdatascience.com/understanding-the-3-most-common-loss-functions-for-machine-learning-regression-23e0ef3e14d3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4e82dd",
   "metadata": {},
   "source": [
    "## Types of Supervised Machine Learning Algorithms\n",
    "\n",
    "Although we'll be focusing on a random forest model in the coding example below, we want to mention other popular algorithims that can be used for supervised machine learning. These include: \n",
    "\n",
    "+ **K-Nearest Neighbors (KNN):** Uses Euclidean distance to classify a data point in the test set based upon the most common class of neighboring data points.\n",
    "<img src=\"https://user-images.githubusercontent.com/96756991/232493057-1e7ce98b-6985-44cd-98a9-3cfea5994659.png\" width=\"684\" />\n",
    "*Created with BioRender.com*\n",
    "\n",
    "+ **Support Vector Machine (SVM):** Creates a decision boundary line (hyperplane) in n-dimensional space to seperate the data into each class so that when new data is presented they can be easily cateogrized. \n",
    "<img src=\"https://user-images.githubusercontent.com/96756991/233735220-08682ea4-fe13-41c8-8ac5-9ddda7859328.png\" width=\"684\" />\n",
    "\n",
    "*Created with BioRender.com* \n",
    "\n",
    "+ **Random Forest:** Uses a multitude of decison trees trained on different parts of the training set and the classification of a data point in the test set is aggregated from all the trees. A **decision tree** \n",
    "\n",
    "\n",
    "### Resources\n",
    "+ [K-Nearst Neighbor](https://www.ibm.com/topics/knn)\n",
    "+ [Support Vector Machine](https://www.javatpoint.com/machine-learning-support-vector-machine-algorithm) **I LIKE THE TEXT IN THIS REFERENCE, BUT IT CONTAINS PYTHON CODE. NOT SURE IF THAT'S CONFUSING.**\n",
    "+ [Random Forest](https://www.ibm.com/in-en/topics/random-forest)\n",
    "\n",
    "Now that we have a better understanding of supervised machine learning, let's go ahead and write the code to predict iAs detection. \n",
    "\n",
    "### Changing Data Types \n",
    "`Water_Sample_Date` needs to be converted from a character to a date type, so that Random Forest understands this column contains dates. This will be done using the `lubridate` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "acf23334",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A data.frame: 6 × 8</caption>\n",
       "<thead>\n",
       "\t<tr><th></th><th scope=col>Tax_ID</th><th scope=col>Water_Sample_Date</th><th scope=col>Casing_Depth</th><th scope=col>Well_Depth</th><th scope=col>Static_Water_Depth</th><th scope=col>Flow_Rate</th><th scope=col>pH</th><th scope=col>Detect_Concentration</th></tr>\n",
       "\t<tr><th></th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;date&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;chr&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>1</th><td>1006004</td><td>2012-09-24</td><td>52</td><td>165</td><td>41</td><td>60.0</td><td>7.7</td><td>ND</td></tr>\n",
       "\t<tr><th scope=row>2</th><td>1024009</td><td>2015-12-17</td><td>40</td><td>445</td><td>42</td><td> 2.0</td><td>7.3</td><td>ND</td></tr>\n",
       "\t<tr><th scope=row>3</th><td>1054019</td><td>2015-02-02</td><td>45</td><td>160</td><td>40</td><td>40.0</td><td>7.4</td><td>ND</td></tr>\n",
       "\t<tr><th scope=row>4</th><td>1057017</td><td>2012-10-22</td><td>42</td><td>440</td><td>57</td><td> 1.5</td><td>8.0</td><td>D </td></tr>\n",
       "\t<tr><th scope=row>5</th><td>1060006</td><td>2011-01-03</td><td>48</td><td>120</td><td>42</td><td>25.0</td><td>7.1</td><td>ND</td></tr>\n",
       "\t<tr><th scope=row>6</th><td>1066006</td><td>2015-12-15</td><td>60</td><td>280</td><td>32</td><td>10.0</td><td>8.2</td><td>D </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A data.frame: 6 × 8\n",
       "\\begin{tabular}{r|llllllll}\n",
       "  & Tax\\_ID & Water\\_Sample\\_Date & Casing\\_Depth & Well\\_Depth & Static\\_Water\\_Depth & Flow\\_Rate & pH & Detect\\_Concentration\\\\\n",
       "  & <chr> & <date> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <chr>\\\\\n",
       "\\hline\n",
       "\t1 & 1006004 & 2012-09-24 & 52 & 165 & 41 & 60.0 & 7.7 & ND\\\\\n",
       "\t2 & 1024009 & 2015-12-17 & 40 & 445 & 42 &  2.0 & 7.3 & ND\\\\\n",
       "\t3 & 1054019 & 2015-02-02 & 45 & 160 & 40 & 40.0 & 7.4 & ND\\\\\n",
       "\t4 & 1057017 & 2012-10-22 & 42 & 440 & 57 &  1.5 & 8.0 & D \\\\\n",
       "\t5 & 1060006 & 2011-01-03 & 48 & 120 & 42 & 25.0 & 7.1 & ND\\\\\n",
       "\t6 & 1066006 & 2015-12-15 & 60 & 280 & 32 & 10.0 & 8.2 & D \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A data.frame: 6 × 8\n",
       "\n",
       "| <!--/--> | Tax_ID &lt;chr&gt; | Water_Sample_Date &lt;date&gt; | Casing_Depth &lt;dbl&gt; | Well_Depth &lt;dbl&gt; | Static_Water_Depth &lt;dbl&gt; | Flow_Rate &lt;dbl&gt; | pH &lt;dbl&gt; | Detect_Concentration &lt;chr&gt; |\n",
       "|---|---|---|---|---|---|---|---|---|\n",
       "| 1 | 1006004 | 2012-09-24 | 52 | 165 | 41 | 60.0 | 7.7 | ND |\n",
       "| 2 | 1024009 | 2015-12-17 | 40 | 445 | 42 |  2.0 | 7.3 | ND |\n",
       "| 3 | 1054019 | 2015-02-02 | 45 | 160 | 40 | 40.0 | 7.4 | ND |\n",
       "| 4 | 1057017 | 2012-10-22 | 42 | 440 | 57 |  1.5 | 8.0 | D  |\n",
       "| 5 | 1060006 | 2011-01-03 | 48 | 120 | 42 | 25.0 | 7.1 | ND |\n",
       "| 6 | 1066006 | 2015-12-15 | 60 | 280 | 32 | 10.0 | 8.2 | D  |\n",
       "\n"
      ],
      "text/plain": [
       "  Tax_ID  Water_Sample_Date Casing_Depth Well_Depth Static_Water_Depth\n",
       "1 1006004 2012-09-24        52           165        41                \n",
       "2 1024009 2015-12-17        40           445        42                \n",
       "3 1054019 2015-02-02        45           160        40                \n",
       "4 1057017 2012-10-22        42           440        57                \n",
       "5 1060006 2011-01-03        48           120        42                \n",
       "6 1066006 2015-12-15        60           280        32                \n",
       "  Flow_Rate pH  Detect_Concentration\n",
       "1 60.0      7.7 ND                  \n",
       "2  2.0      7.3 ND                  \n",
       "3 40.0      7.4 ND                  \n",
       "4  1.5      8.0 D                   \n",
       "5 25.0      7.1 ND                  \n",
       "6 10.0      8.2 D                   "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "well_data = well_data %>%\n",
    "    # converting water sample date from a character to a date type \n",
    "    mutate(Water_Sample_Date = mdy(Water_Sample_Date))\n",
    "\n",
    "head(well_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be60453a",
   "metadata": {},
   "source": [
    "### Testing for differences in predictor variables acrosss the outcome classes\n",
    "\n",
    "It's useful to run summary statistics on the variables that will be used as predictors in the model to see if there are differences in distributions between the outcomes classes (either non-detect or detect in this case). Typically, greater signficance often leads to better predictivity, since the model is better able to separate the classes. We'll use the `tbl_summary()` function from the `gtsummary` package.\n",
    "\n",
    "For more information on the `tbl_summary()` function, check out this helpful [Tutorial](https://www.danieldsjoberg.com/gtsummary/articles/tbl_summary.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3538a5a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A tibble: 6 × 5</caption>\n",
       "<thead>\n",
       "\t<tr><th scope=col>**Characteristic**</th><th scope=col>**N**</th><th scope=col>**D**, N = 198</th><th scope=col>**ND**, N = 515</th><th scope=col>**p-value**</th></tr>\n",
       "\t<tr><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;chr&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><td>Water_Sample_Date </td><td>713</td><td>2013-03-05 (957.843005291701)</td><td>2013-06-05 (979.174260670888)</td><td>0.3   </td></tr>\n",
       "\t<tr><td><span style=white-space:pre-wrap>Casing_Depth      </span></td><td>713</td><td><span style=white-space:pre-wrap>55 (23)                      </span></td><td><span style=white-space:pre-wrap>74 (33)                      </span></td><td>&lt;0.001</td></tr>\n",
       "\t<tr><td>Well_Depth        </td><td>713</td><td>334 (128)                    </td><td>301 (144)                    </td><td>0.005 </td></tr>\n",
       "\t<tr><td>Static_Water_Depth</td><td>713</td><td>36 (13)                      </td><td>35 (12)                      </td><td>0.4   </td></tr>\n",
       "\t<tr><td><span style=white-space:pre-wrap>Flow_Rate         </span></td><td>713</td><td><span style=white-space:pre-wrap>14 (16)                      </span></td><td><span style=white-space:pre-wrap>25 (33)                      </span></td><td>&lt;0.001</td></tr>\n",
       "\t<tr><td><span style=white-space:pre-wrap>pH                </span></td><td>713</td><td><span style=white-space:pre-wrap>7.82 (0.40)                  </span></td><td><span style=white-space:pre-wrap>7.45 (0.55)                  </span></td><td>&lt;0.001</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A tibble: 6 × 5\n",
       "\\begin{tabular}{lllll}\n",
       " **Characteristic** & **N** & **D**, N = 198 & **ND**, N = 515 & **p-value**\\\\\n",
       " <chr> & <chr> & <chr> & <chr> & <chr>\\\\\n",
       "\\hline\n",
       "\t Water\\_Sample\\_Date  & 713 & 2013-03-05 (957.843005291701) & 2013-06-05 (979.174260670888) & 0.3   \\\\\n",
       "\t Casing\\_Depth       & 713 & 55 (23)                       & 74 (33)                       & <0.001\\\\\n",
       "\t Well\\_Depth         & 713 & 334 (128)                     & 301 (144)                     & 0.005 \\\\\n",
       "\t Static\\_Water\\_Depth & 713 & 36 (13)                       & 35 (12)                       & 0.4   \\\\\n",
       "\t Flow\\_Rate          & 713 & 14 (16)                       & 25 (33)                       & <0.001\\\\\n",
       "\t pH                 & 713 & 7.82 (0.40)                   & 7.45 (0.55)                   & <0.001\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A tibble: 6 × 5\n",
       "\n",
       "| **Characteristic** &lt;chr&gt; | **N** &lt;chr&gt; | **D**, N = 198 &lt;chr&gt; | **ND**, N = 515 &lt;chr&gt; | **p-value** &lt;chr&gt; |\n",
       "|---|---|---|---|---|\n",
       "| Water_Sample_Date  | 713 | 2013-03-05 (957.843005291701) | 2013-06-05 (979.174260670888) | 0.3    |\n",
       "| Casing_Depth       | 713 | 55 (23)                       | 74 (33)                       | &lt;0.001 |\n",
       "| Well_Depth         | 713 | 334 (128)                     | 301 (144)                     | 0.005  |\n",
       "| Static_Water_Depth | 713 | 36 (13)                       | 35 (12)                       | 0.4    |\n",
       "| Flow_Rate          | 713 | 14 (16)                       | 25 (33)                       | &lt;0.001 |\n",
       "| pH                 | 713 | 7.82 (0.40)                   | 7.45 (0.55)                   | &lt;0.001 |\n",
       "\n"
      ],
      "text/plain": [
       "  **Characteristic** **N** **D**, N = 198               \n",
       "1 Water_Sample_Date  713   2013-03-05 (957.843005291701)\n",
       "2 Casing_Depth       713   55 (23)                      \n",
       "3 Well_Depth         713   334 (128)                    \n",
       "4 Static_Water_Depth 713   36 (13)                      \n",
       "5 Flow_Rate          713   14 (16)                      \n",
       "6 pH                 713   7.82 (0.40)                  \n",
       "  **ND**, N = 515               **p-value**\n",
       "1 2013-06-05 (979.174260670888) 0.3        \n",
       "2 74 (33)                       <0.001     \n",
       "3 301 (144)                     0.005      \n",
       "4 35 (12)                       0.4        \n",
       "5 25 (33)                       <0.001     \n",
       "6 7.45 (0.55)                   <0.001     "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "well_data %>%\n",
    "    tbl_summary(by = Detect_Concentration,\n",
    "    # Selecting columns to include\n",
    "    include = colnames(well_data[c(2:8)]), \n",
    "    # Displaying the mean and standard deviation in parantheses for all continuous variables\n",
    "                statistic = list(all_continuous() ~ \"{mean} ({sd})\")) %>%\n",
    "    # Adding a column that displays the total number of samples for each variable\n",
    "    # This will be 713 for all variables since we have no missing data\n",
    "    add_n() %>% \n",
    "    # Adding a column that dispalys the p value from anova\n",
    "    add_p(test = list(all_continuous() ~ \"aov\")) %>% \n",
    "    as_tibble()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38577cf1",
   "metadata": {},
   "source": [
    "All the variables are very signficant with the exception of the sample date and the static water depth, therefore the model should predict fairly well.\n",
    "\n",
    "## Setting up Cross Validation\n",
    "\n",
    "As mentioned above, cross validation is implemented so that the model is trained and tested on different portions of the entire dataset. We'll use 5-fold cross validation, but note that *k* can be changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "64cff629",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the splits in the dataset are random, a seed is set for reproducibility to ensure the splits are occuring\n",
    "# in the same locations each time the code is run\n",
    "set.seed(12)\n",
    "\n",
    "# 5-fold cross validation\n",
    "# Saving the index (row number) where the 5 splits are occuring\n",
    "# These indices will be iterated through using a loop to create each training and testing datasets\n",
    "well_index = createFolds(well_data$Detect_Concentration, k = 5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72462a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an empty dataframe to save the metics\n",
    "metrics = data.frame()\n",
    "\n",
    "# Iterating through the cross validation folds\n",
    "for (i in 1:length(well_index)){\n",
    "    # Training data\n",
    "    data_train = well_data[-well_index[[i]],]\n",
    "    \n",
    "    # Test data\n",
    "    data_test = well_data[well_index[[i]],]\n",
    "    \n",
    "    # Random forest\n",
    "    reg_rf <- randomForest(as.formula(paste0(outcome, \"~.\")), data = balanced_data_train,\n",
    "                               ntree = rf_error_df$Tree.Number[min(best_oob_errors)],\n",
    "                               mtry = rf_error_df$Variable.Number[min(best_oob_errors)])\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3cc5b0",
   "metadata": {},
   "source": [
    "## Working with the data \n",
    "\n",
    "**Download Packages**\n",
    "```{r}\n",
    "install.packages(\"tidyverse\")\n",
    "install.packages(\"pheatmap\")\n",
    "install.packages(\"ggplot2\")\n",
    "install.packages(\"reshape2\")\n",
    "install.packages(\"arsenal\")\n",
    "install.packages(\"superheat\")\n",
    "\n",
    "library(tidyverse)\n",
    "library(pheatmap)\n",
    "library(ggplot2)\n",
    "library(reshape2)\n",
    "library(arsenal)\n",
    "library(superheat)\n",
    "library(readxl)\n",
    "library(gtsummary)\n",
    "library(e1071)\n",
    "library(Hmisc)\n",
    "library(glmnet)\n",
    "library(randomForest)\n",
    "library(pROC)\n",
    "library(lattice)\n",
    "library(survival)\n",
    "library(Formula)\n",
    "library(caret)\n",
    "```\n",
    "**Set working directory**\n",
    "```{r}\n",
    "getwd()\n",
    "setwd(\"/Users/ritaavenbuan/Desktop\")\n",
    "```\n",
    "\n",
    "**Read in the Data & change sex into factors**\n",
    "\n",
    "Changing Sex into factors will help the run the machine learning models run properly.\n",
    "\n",
    "```{r}\n",
    "#import the data and remove variables that are not going to be useful for your analysis\n",
    "pre.dataset <- na.omit(read.csv(\"Proteomics_Imputed_PreExposureSubjects.csv\")) %>%\n",
    "  select(-SubjectID, -Race, -Ethnicity, -Age, -BMI) %>%\n",
    "  drop_na() \n",
    "\n",
    "#change the class of Sex (from characters to factors)\n",
    "class(pre.dataset$Sex)\n",
    "pre.dataset$Sex <- as.factor(pre.dataset$Sex)\n",
    "```\n",
    "\n",
    "**Statistical Summary of the Data**\n",
    "\n",
    "```{r}\n",
    "pre.dataset %>%\n",
    "  tbl_summary(by = \"Sex\", \n",
    "               statistic = list(all_continuous() ~ \"{mean} ({sd})\")) %>%\n",
    "  add_n() %>%\n",
    "  add_p(test = list(all_continuous() ~ \"aov\")) %>%\n",
    "  as_flex_table()\n",
    "\n",
    "\n",
    "#post.dataset <-read.csv(\"Proteomics_Imputed_PostExposureSubjects.csv\")\n",
    "```\n",
    "\n",
    "**Review the data**\n",
    "\n",
    "```{r}\n",
    "head(pre.dataset)\n",
    "```\n",
    "\n",
    "**Decision Tree**\n",
    "\n",
    "A Decision Tree is a type of supervised machine learning model that makes predictions based on how a question was answered. \n",
    "\n",
    "![image](https://user-images.githubusercontent.com/96756991/228426001-9a73d4b5-017c-430b-b48f-0aa91dedc4ea.png)\n",
    "Created with BioRender.com\n",
    "\n",
    "- _Root node:_ Base of the Decision Tree \n",
    "- _Splitting:_ A node divided into sub-nodes \n",
    "- _Decision mode:_ A sub-node broken down into additional sub-nodes\n",
    "- _Leaf node:_ A sub-node that does not split into additional sub-nodes (terminal node). It indicates a potential outcome. \n",
    "- _Pruning:_ Removing sub-nodes of a decision tree\n",
    "- _Branch:_ A section of a decision tree with multiple nodes\n",
    "- _Pruning:_ The process of removing sub-nodes of a decision tree\n",
    "\n",
    "In this module, we will start by creating decision tree. \n",
    "\n",
    "[Reference](https://www.mastersindatascience.org/learning/machine-learning-algorithms/decision-tree/)\n",
    "\n",
    "**Set data up for Reproducibility**\n",
    "```{r}\n",
    "\n",
    "#set up for reproducibility \n",
    "set.seed(15)\n",
    "control_params <- rpart.control(minsplit = 5)\n",
    "\n",
    "```\n",
    "\n",
    "**Splitting data into testing and training set**\n",
    "\n",
    "In this step we want to split the data into the training set (what the algorithm will use to learn data) and the test set. We also want to make sure that we are cross-validating here. \n",
    "\n",
    "_Cross-validate:_ The ability for the machine to predict new data and test its accuracy. We want to make sure our model is not overfitting (giving accurate results for predictions for training data but not new data). \n",
    "[Reference](https://learn.g2.com/cross-validation)\n",
    "\n",
    "```{r}\n",
    "sex_data_index = createFolds(pre.dataset$Sex, k = 5) #K in Cross Validation is usually 5 or 10 \n",
    "errors = data.frame()\n",
    "for (i in 1:length(sex_data_index)){\n",
    "  sex_train = pre.dataset[-sex_data_index[[i]],]\n",
    "  sex_test = pre.dataset[sex_data_index[[i]],]\n",
    "  \n",
    "  reg_tree = rpart(Sex ~., data = sex_train, method = \"class\", control = control_params)\n",
    "  \n",
    "  vim <- varImp(reg_tree)\n",
    "  \n",
    "  pred_tree <- predict(reg_tree, sex_test, type = \"class\", na.action = na.pass)\n",
    "  \n",
    "  cm_tree <- confusionMatrix(pred_tree, sex_test$Sex)\n",
    "  \n",
    "  accuracy_tree <- cm_tree$overall[\"Accuracy\"]\n",
    "}\n",
    "```\n",
    "\n",
    "**Preform Confusion Matrix**\n",
    "\n",
    "\n",
    "```{r}\n",
    "#perform confusion matrix \n",
    "# Set the number of folds for cross-validation\n",
    "k <- 5\n",
    "\n",
    "# Create the folds\n",
    "folds <- cut(seq(1, nrow(pre.dataset)), breaks = k, labels = FALSE)\n",
    "\n",
    "# Initialize an empty list to store the evaluation metrics for each fold\n",
    "eval_metrics <- list()\n",
    "\n",
    "# Loop over each fold\n",
    "for (i in 1:k) {\n",
    "  \n",
    "  # Split the data into training and validation sets for this fold\n",
    "  train_indices <- which(folds != i)\n",
    "  valid_indices <- which(folds == i)\n",
    "  sex_train <- pre.dataset[train_indices, ]\n",
    "  sex_valid <- pre.dataset[valid_indices, ]\n",
    "  \n",
    "  # Fit the decision tree model on the training set\n",
    "  reg_tree <- rpart(Sex ~., data = sex_train, method = \"class\", control = rpart.control(minsplit = 5))\n",
    "  \n",
    "  # Make predictions on the validation set\n",
    "  pred_tree <- predict(reg_tree, sex_valid, type = \"class\")\n",
    "  \n",
    "  # Compute the evaluation metrics for this fold\n",
    "  cm_tree <- confusionMatrix(pred_tree, sex_valid$Sex)\n",
    "  eval_metrics[[i]] <- c(accuracy = cm_tree$overall['Accuracy'], \n",
    "                         sensitivity = cm_tree$byClass['Sensitivity'],\n",
    "                         specificity = cm_tree$byClass['Specificity'])\n",
    "}\n",
    "\n",
    "# Calculate the average evaluation metrics across all folds\n",
    "eval_metrics <- do.call(rbind, eval_metrics)\n",
    "avg_eval_metrics <- colMeans(eval_metrics)\n",
    "\n",
    "#confusion matrix results\n",
    "print(cm_tree)\n",
    "```\n",
    "```{r}\n",
    "Confusion Matrix and Statistics\n",
    "\n",
    "          Reference\n",
    "Prediction F M\n",
    "         F 2 0\n",
    "         M 3 1\n",
    "                                          \n",
    "               Accuracy : 0.5             \n",
    "                 95% CI : (0.1181, 0.8819)\n",
    "    No Information Rate : 0.8333          \n",
    "    P-Value [Acc > NIR] : 0.9913          \n",
    "                                          \n",
    "                  Kappa : 0.1818          \n",
    "                                          \n",
    " Mcnemar's Test P-Value : 0.2482          \n",
    "                                          \n",
    "            Sensitivity : 0.4000          \n",
    "            Specificity : 1.0000          \n",
    "         Pos Pred Value : 1.0000          \n",
    "         Neg Pred Value : 0.2500          \n",
    "             Prevalence : 0.8333          \n",
    "         Detection Rate : 0.3333          \n",
    "   Detection Prevalence : 0.3333          \n",
    "      Balanced Accuracy : 0.7000          \n",
    "                                          \n",
    "       'Positive' Class : F               \n",
    "```\n",
    "\n",
    "Based on our confusion matrix, we have 50% accuracy which means that our model is accurately classifying proteins based on sex 50% of the time.  \n",
    "\n",
    "**Visualize the Decision Tree**\n",
    "\n",
    "We want to visualize the decision tree for the training data created above to see how the model is using the protemics data to categorize sex.\n",
    "\n",
    "```{r}\n",
    "reg_tree\n",
    "rpart.plot(reg_tree, type = 2, extra = 101, under = TRUE, fallen.leaves = TRUE, main = \"Decision Tree for Protein Differences by Sex\")\n",
    "\n",
    "cm_tree\n",
    "\n",
    "table_matrix <- table(sex_test$Sex, pred_tree)\n",
    "table_matrix\n",
    "```\n",
    "![image](https://user-images.githubusercontent.com/96756991/232521526-56740254-ad7e-4356-9d57-aa2c85914205.png)\n",
    "\n",
    "Based on this decision tree, a classified protein in our proteomics training data set is CFP. When it is greater than 5540, the model predicts that the sex is female; however, when it is less than 5540, it uses another protein called BPIFB4. When BPIFB4 is greater than or equal to 6821, the model predicts the sex is female; else, it is male. \n",
    "\n",
    "Now, we are going to now run a more robust model Random Foreset to see if our accuracy increases and to see which proteins best predict sex. \n",
    "\n",
    "**Random Forest**\n",
    "\n",
    "Random Forest creates many decision trees to reach an answer about the data set. \n",
    "\n",
    "![image](https://user-images.githubusercontent.com/96756991/228436169-c7fe932a-9aa4-41e2-b56c-9a8830b834a8.png)\n",
    "Created with BioRender.com\n",
    "\n",
    "[Reference](https://www.ibm.com/topics/random-forest) \n",
    "\n",
    "**Set data up for Reproducibility**\n",
    "```{r}\n",
    "rf_classification = function(pre.dataset, outcome, pred_outcome) {\n",
    "  #setting for reproducibility\n",
    "  set.seed(15)\n",
    "\n",
    "```\n",
    "\n",
    "**Split data into training and test sets**\n",
    "```{r}\n",
    "  #splitting data into training and testing sets \n",
    "  sex_data_index = createFolds(pre.dataset$Sex, k = 5) #K in Cross Validation is usually 5 or 10 \n",
    "```\n",
    "\n",
    "**Create an empty data frame for error, empty numeric vector for accuracy, sensitivity and specificity, and an empty list for importance**\n",
    "```{r}\n",
    "\n",
    "  errors = data.frame()\n",
    "  accuracy = c()\n",
    "  sensitivity = c()\n",
    "  specificity = c()\n",
    "  importance = list()\n",
    " ```\n",
    " \n",
    " **Run Random Forest and Cross-validation**\n",
    " ```{r}\n",
    "  #set up cross valdiation loop using the sex_data_index created previously\n",
    "  for (i in 1:length(sex_data_index)){\n",
    "    sex_train = pre.dataset[-sex_data_index[[i]],]\n",
    "    sex_test = pre.dataset[sex_data_index[[i]],]\n",
    "    \n",
    "    # Set up the grid of hyperparameters to search over\n",
    "    ntree_values = c(50, 250, 500) #number of trees\n",
    "    p = dim(pre.dataset)[2] - 1 #number of variables in dataset\n",
    "    mtry_values = c(p/2, sqrt(p), p)\n",
    "    \n",
    "    # Set up the tuning grid for the random forest\n",
    "    tuning_grid = expand.grid(ntree = ntree_values,\n",
    "                              mtry = mtry_values)\n",
    "    \n",
    "    # Fit the random forest model with 5-fold cross-validation\n",
    "    rf_model = train(as.factor(outcome) ~ ., \n",
    "                     data = sex_train, \n",
    "                     method = \"rf\", \n",
    "                     trControl = trainControl(method = \"cv\", \n",
    "                                              number = 5), \n",
    "                     tuneGrid = tuning_grid)\n",
    "    \n",
    "    # Make predictions on the test set\n",
    "    rf_pred = predict(rf_model, newdata = sex_test)\n",
    "    \n",
    "    # Run confusion matrix for the test set\n",
    "    cm = confusionMatrix(rf_pred, sex_test$Sex)\n",
    "    accuracy[i] = cm$overall[1]\n",
    "    sensitivity[i] = cm[2,2]/sum(cm[2,])\n",
    "    specificity[i] = cm[1,1]/sum(cm[1,])\n",
    "    \n",
    "    # Save the variable importance for this fold\n",
    "    importance[[i]] = varImp(rf_model)$importance\n",
    "    \n",
    "  }\n",
    "  \n",
    "  # Combine the evaluation metrics across all folds\n",
    "  eval_metrics = data.frame(accuracy = mean(accuracy), \n",
    "                            sensitivity = mean(sensitivity), \n",
    "                            specificity = mean(specificity))\n",
    "  \n",
    "  # Run the mean importance of each variable across all folds\n",
    "  variable_importance = data.frame(variable = names(pre.dataset)[-pred_outcome], \n",
    "                                   mean_importance = unlist(lapply(importance, \n",
    "                                                                   function(x) mean(x[,\"MeanDecreaseGini\"]))))\n",
    "  \n",
    "  # Return the evaluation metrics and variable importance\n",
    "  return(list(eval_metrics = eval_metrics, variable_importance = variable_importance))\n",
    "}\n",
    "```\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/96756991/232552051-a6016e75-fd91-45fd-b237-056f1e1a56ea.png)\n",
    "\n",
    "**Mean decrease accuracy**\n",
    "\n",
    "Mean decrease accuracy tells us how important each feature is in making accurate predictions. A higher mean decrease accuracy score indicates that a feature is more important in making predictions, while a lower score means it's less important. With this particular dataset, we see that proteins such as DCXR, GBP6, HPR, PSME1, DNAJA4, GSTP1, DDT, KRT7, CTNNA1, and KRT18 are proteins that best predict sex differences.\n",
    "[Reference](https://plos.figshare.com/articles/figure/Variable_importance_plot_mean_decrease_accuracy_and_mean_decrease_Gini_/12060105/1)\n",
    "\n",
    "**Mean decrease Gini**\n",
    "\n",
    "The mean decrease in Gini is based on the decrease in the Gini impurity index, and it measures how each predictor contributes to the purity of the nodes in the decision trees. In other words, the mean decrease Gini score tells us how important a feature is in making decisions in the dataset. The higher the score, the more important the feature is in making decisions and splitting the data. Our data shows that HPR, DCXR, PSMB7, KRT15, HYOU1, and GBP6 are essential proteins for making decisions in this dataset.\n",
    "[Reference](https://www.analyticsvidhya.com/blog/2021/03/how-to-select-best-split-in-decision-trees-gini-impurity/)\n",
    "\n",
    "**Random Forest Confusion Matrix**\n",
    "\n",
    "```{r}\n",
    "# Create folds for cross-validation\n",
    "folds <- createFolds(pre.dataset$Sex, k = 5)\n",
    "\n",
    "# Initialize empty vectors for storing results\n",
    "accuracy <- c()\n",
    "sensitivity <- c()\n",
    "specificity <- c()\n",
    "\n",
    "# Loop through each fold\n",
    "for (i in 1:length(folds)) {\n",
    "  # Split data into training and test sets\n",
    "  train_data <- pre.dataset[-folds[[i]], ]\n",
    "  test_data <- pre.dataset[folds[[i]], ]\n",
    "  \n",
    "  # Train random forest model on training data\n",
    "  rf_model <- randomForest(Sex ~ ., data = train_data, importance = TRUE)\n",
    "  \n",
    "  # Make predictions on test data\n",
    "  predictions <- predict(rf_model, test_data)\n",
    "  \n",
    "  # Calculate confusion matrix\n",
    "  cm <- confusionMatrix(predictions, test_data$Sex)\n",
    "  cm\n",
    "  \n",
    "  # Calculate and store metrics\n",
    "  accuracy[i] <- cm$overall[\"Accuracy\"]\n",
    "  sensitivity[i] <- cm$byClass[\"Sensitivity\"]\n",
    "  specificity[i] <- cm$byClass[\"Specificity\"]\n",
    "}\n",
    "\n",
    "# Print mean and standard deviation of metrics\n",
    "cat(\"Mean Accuracy:\", mean(accuracy), \"±\", sd(accuracy), \"\\n\")\n",
    "cat(\"Mean Sensitivity:\", mean(sensitivity), \"±\", sd(sensitivity), \"\\n\")\n",
    "cat(\"Mean Specificity:\", mean(specificity), \"±\", sd(specificity), \"\\n\")\n",
    "\n",
    "```\n",
    "\n",
    "```{r}\n",
    "Mean Accuracy: 0.4933333 ± 0.2349941 \n",
    "Mean Sensitivity: 0.2666667 ± 0.2527625 \n",
    "Mean Specificity: 0.65 ± 0.3555122 \n",
    "```\n",
    "\n",
    "By comparing the mean accuracy of the Random Forest Model to the Decision Tree, we find that the Decision Tree achieved higher accuracy with this dataset. This indicates that, for this specific dataset, a Decision Tree would be the more appropriate model to use for predicting our data. Considering the relatively low number of observations in the dataset (27), we may not have sufficient data to support the use of a robust Random Forest model.\n",
    "\n",
    "\n",
    "## Test Your Knowledge \n",
    "\n",
    "Using the Post-exposure data provided, create a decision tree and random forest model to answer the following questions:\n",
    "\n",
    "1. Can we predict sex based on protein expression?\n",
    "2. Which proteins best predict sex? \n",
    "3. Which model is more appropriate for predicting the first two outcomes? \n",
    "\n",
    "\n",
    "## Conclusion \n",
    "\n",
    "In conclusion, this training module has provided an informative introduction to supervised machine learning using classification techniques in R. Machine learning is a powerful tool that can help researchers gain new insights and improve models to analyze complex datasets better. The example we've explored demonstrates the utility of machine learning models for better understanding data with numerous features.\n",
    "\n",
    "This module also offers valuable resources that can serve as a guide to learning more about machine learning and its application to environmental health science research. By leveraging machine learning techniques, researchers can unlock new avenues for exploration and contribute to a deeper understanding of complex environmental health issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae87403",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.1.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
